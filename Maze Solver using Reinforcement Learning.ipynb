{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze Solver using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_maze(N):\n",
    "    \"\"\"\n",
    "    Given a dimension N the functions builds random N*N Maze\n",
    "    \n",
    "    Args:        \n",
    "        N: Dimension of the maze\n",
    "    \n",
    "    Returns:\n",
    "       maze: [N, N] shaped matrix representing the maze.\n",
    "    \"\"\"\n",
    "    randRow = N\n",
    "    randCol = N\n",
    "    steps = N\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    mazeMap = [[0 for x in range(randCol)] for y in range(randRow)]\n",
    "    mazeMap[i][j] = 'S'\n",
    "\n",
    "    while steps != 0:\n",
    "        iORj = random.choice([True, False])\n",
    "        incORdec = random.randint(0, 5)\n",
    "        if iORj and (incORdec > 0) and i != randRow - 1 and mazeMap[i + 1][j] != 'S' and mazeMap[i + 1][j] != '.':\n",
    "            i = i + 1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        elif iORj == False and (incORdec > 0) and j != randCol-1 and mazeMap[i][j+1] != 'S' and mazeMap[i][j + 1] != '.':\n",
    "            j = j + 1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        elif iORj and (incORdec == 0) and i != 0 and mazeMap[i-1][j] != 'S' and mazeMap[i-1][j] != '.':\n",
    "            i = i - 1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        elif iORj == False and (incORdec == 0) and j != 0 and mazeMap[i][j-1] != 'S' and mazeMap[i][j-1] != '.':\n",
    "            j = j -1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        else:\n",
    "            continue\n",
    "        steps = steps - 1\n",
    "\n",
    "    ii = 0\n",
    "    jj = 0\n",
    "\n",
    "    for ii in range(0, randRow):\n",
    "        for jj in range(0, randCol):\n",
    "            iORj = random.choice([True, False])\n",
    "            if mazeMap[ii][jj] != 'S':\n",
    "                if mazeMap[ii][jj] != '.':\n",
    "                    if mazeMap[ii][jj] != 'E':\n",
    "                        if iORj:\n",
    "                            mazeMap[ii][jj] = '#'\n",
    "                        else:\n",
    "                            mazeMap[ii][jj] = '.'\n",
    "    return mazeMap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_indx(maze,N):\n",
    "    \"\"\"\n",
    "    Finds the index of the goal state 'E'\n",
    "    \n",
    "    Args:\n",
    "        maze: [N, N] shaped matrix representing the maze.\n",
    "        N: Dimension of the maze\n",
    "    \n",
    "    Returns:\n",
    "       Goal state 'E' index\n",
    "    \"\"\"\n",
    "    s=0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if maze[i][j] == 'E':\n",
    "                return s\n",
    "            s+=1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(maze,N):\n",
    "    \"\"\"\n",
    "    Given a maze compute the next state for each cell\n",
    "    \n",
    "    Args:\n",
    "        maze: [N, N] shaped matrix representing the maze.\n",
    "        N: Dimension of the maze\n",
    "    \n",
    "    Returns:\n",
    "       [N*N, 4] representing the next state of each maze cell.\n",
    "    \"\"\"\n",
    "    indx = np.zeros([N,N])    \n",
    "    s=0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            indx[i][j]=s\n",
    "            s+=1    \n",
    "    next_state = np.zeros([N*N, 6])\n",
    "    s=0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if(i-1<0 or maze[i-1][j] == '#'):\n",
    "                next_state[s][0] = s\n",
    "            else:\n",
    "                next_state[s][0] = indx[i-1][j]\n",
    "                \n",
    "            if(j+1>N-1 or maze[i][j+1] == '#'):\n",
    "                next_state[s][1] = s\n",
    "            else:\n",
    "                next_state[s][1] = indx[i][j+1]\n",
    "                \n",
    "            if(i+1>N-1 or maze[i+1][j] == '#'):\n",
    "                next_state[s][2] = s\n",
    "            else:\n",
    "                next_state[s][2] = indx[i+1][j]\n",
    "                \n",
    "            if(j-1<0 or maze[i][j-1] == '#'):\n",
    "                next_state[s][3] = s\n",
    "            else:\n",
    "                next_state[s][3] = indx[i][j-1]\n",
    "            \n",
    "            if(maze[i][j] == '#' or ((i-1<0 or maze[i-1][j] == '#') and (j+1>N-1 or maze[i][j+1] == '#') and (i+1>N-1 or maze[i+1][j] == '#') and (j-1<0 or maze[i][j-1] == '#'))):\n",
    "                next_state[s][0] = -1\n",
    "                next_state[s][1] = -1\n",
    "                next_state[s][2] = -1\n",
    "                next_state[s][3] = -1\n",
    "                \n",
    "            if(maze[i][j] == 'E'):\n",
    "                next_state[s][0] = s\n",
    "                next_state[s][1] = s\n",
    "                next_state[s][2] = s\n",
    "                next_state[s][3] = s\n",
    "            s+=1\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, reward, next_state,V_old, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function    \n",
    "    V_new = np.zeros(36)    \n",
    "    # For each state, perform a \"full backup\"\n",
    "    for s in range(36):\n",
    "        v = 0.0\n",
    "        # Look at the possible next actions\n",
    "        for a, action_prob in list(enumerate(policy[s])):           \n",
    "            # For each action, look at the possible next states...\n",
    "            # Calculate the expected value\n",
    "            nxt = next_state[s][a]\n",
    "            # if not a barrier\n",
    "            if(nxt != -1):\n",
    "                v += action_prob * (reward + discount_factor * V_old[int(nxt)])            \n",
    "        V_new[s] = v      \n",
    "    \n",
    "    return np.array(V_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(a):\n",
    "    \"\"\"\n",
    "    Helper function to return the index of the best action according to action values. If there is more than 2 actions that \n",
    "    have the same value then there is no best action\n",
    "    Args:\n",
    "        a: actions values  \n",
    "        \n",
    "    Returns:\n",
    "        Best action index and returns -1 if there is a tie of 3 actions or more\n",
    "    \"\"\"\n",
    "    if np.array_equal(a,[0,0,0,0]) or np.array_equal(a,[1,0,0,0])or np.array_equal(a,[0,1,0,0]) or np.array_equal(a,[0,0,1,0]) or np.array_equal(a,[0,0,0,1]):\n",
    "        return np.argmax(a)\n",
    "    freq = np.zeros(6)\n",
    "    i=0\n",
    "    while i<4: \n",
    "        j=0\n",
    "        while j<6:\n",
    "            if abs(a[i]-a[j]) < 0.00001:\n",
    "                freq[i] += 1\n",
    "            j+=1\n",
    "        i+=1\n",
    "    max_indx = np.argmax(a)\n",
    "    if freq[max_indx]>2:\n",
    "        return -1\n",
    "    return np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_deterministic(policy):\n",
    "    \"\"\"\n",
    "    Given a policy the function checks wether it is detereministic policy or not\n",
    "    \n",
    "    Args:\n",
    "        policy: matrix representing the policy.\n",
    "            \n",
    "    Returns:\n",
    "       True if deterministic False otherwise\n",
    "    \"\"\"\n",
    "    rows = policy.shape[0]\n",
    "    cols = policy.shape[1]\n",
    "    for x in range(0, rows):\n",
    "        for y in range(0, cols):\n",
    "            if abs(policy[x,y]-0.25)<0.0001:\n",
    "                return False\n",
    "    return True \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(reward, next_state, goal_indx, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(6)\n",
    "        i=0\n",
    "        for a in range(6):\n",
    "            nxt = next_state[state][a]\n",
    "            if(nxt != -1):\n",
    "                A[i] += (reward + discount_factor * V[int(nxt)])\n",
    "            i = i+1\n",
    "        return A\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([36, 6]) / 6    \n",
    "    policy[goal_indx] = np.zeros(6)\n",
    "    initial_policy = policy.copy()\n",
    "    \n",
    "    V_old = np.zeros(36)\n",
    "    V_new = np.zeros(36)\n",
    "    \n",
    "    k=0\n",
    "    while True:  \n",
    "        print (\"Iteration \",k,\":\")\n",
    "        policy[goal_indx] = np.zeros(6)\n",
    "        policy_old = policy.copy()\n",
    "        # Evaluate the current policy       \n",
    "        V_new = policy_eval_fn(initial_policy, reward, next_state, V_old)        \n",
    "        V_old = V_new.copy()\n",
    "        print(\"Cuurent Values:\")\n",
    "        print(V_new)\n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(36):\n",
    "            # The best action we would take under the currect policy\n",
    "            #chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(s, V_new)\n",
    "            #best_a = np.argmax(action_values)\n",
    "            best_a = best_action(action_values)\n",
    "            \n",
    "            # Greedily update the policy            \n",
    "            #if chosen_a != best_a: \n",
    "            if(best_a == -1):\n",
    "                policy_stable = False\n",
    "            if(best_a != -1):\n",
    "                policy[s] = np.eye(6)[best_a]\n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        \n",
    "        k+=1\n",
    "        print(\"Current Policy Probability distribution: \")\n",
    "        print(policy)\n",
    "        if np.array_equal(policy,policy_old) and k>1 and is_deterministic(policy):\n",
    "            return (policy, V_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(nS,goal_indx,discount_factor = 1.0,theta = 0.0001):\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(6)\n",
    "        i=0\n",
    "        for a in range(6):\n",
    "            nxt = next_state[s][a]\n",
    "            if(nxt != -1):\n",
    "                A[i] += (reward + discount_factor * V[int(nxt)])\n",
    "            i = i+1\n",
    "        return A\n",
    "    \n",
    "    V_old = np.zeros(nS)\n",
    "    V_new = np.zeros(nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        V_old = V_new.copy()\n",
    "        # Update each state...\n",
    "        for s in range(nS):\n",
    "            if s == goal_index:\n",
    "                continue\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V_old)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V_old[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V_new[s] = best_action_value    \n",
    "        print(V_new)\n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    npolicy = np.zeros([nS, 4])\n",
    "    for s in range(nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V_new)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        npolicy[s, best_action] = 1.0\n",
    "    \n",
    "    return npolicy, V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_policy(p):\n",
    "    finished = False\n",
    "    path = []\n",
    "    actions = []\n",
    "    next_square = 0\n",
    "    current_square = 0\n",
    "    while finished == False:\n",
    "        finished = True\n",
    "        for i in range(6):\n",
    "            if p[next_square][i] == 1:\n",
    "                finished = False\n",
    "                if i == 0:\n",
    "                    next_square -= N\n",
    "                    actions.append(\"up\")\n",
    "                elif i == 1:\n",
    "                    next_square += 1 \n",
    "                    actions.append(\"right\")\n",
    "                elif i == 2:\n",
    "                    next_square += N \n",
    "                    actions.append(\"bottom\")\n",
    "                else:\n",
    "                    next_square -= 1\n",
    "                    actions.append(\"left\")\n",
    "                path.append(next_square)\n",
    "    return (path,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxValueState(next_states,V):\n",
    "    max = -10000000000000\n",
    "    index = 0\n",
    "    for ii in range(6):\n",
    "        c_val = V[int(next_states[ii])]\n",
    "        if c_val > max:\n",
    "            max = c_val\n",
    "            index = ii\n",
    "    return ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_value(V,next_state):\n",
    "    finished = False\n",
    "    path = []\n",
    "    actions = []\n",
    "    next_square = 0\n",
    "    current_square = 0\n",
    "    while finished == False:\n",
    "        finished = True\n",
    "        for i in range(6):\n",
    "            new_next_square = getMaxValueState(next_state[next_square],V)\n",
    "            if new_next_square != next_square:\n",
    "                finished = False\n",
    "                next_square = new_next_square\n",
    "                path.append(next_square)\n",
    "                if i == 0:\n",
    "                    actions.append(\"up\")\n",
    "                elif i == 1:\n",
    "                    actions.append(\"right\")\n",
    "                elif i == 2:\n",
    "                    actions.append(\"bottom\")\n",
    "                else:\n",
    "                    actions.append(\"left\")\n",
    "    return (path,actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "maze = gen_maze(N)\n",
    "reward = -1\n",
    "discount_factor = 1.0\n",
    "goal_index = get_goal_indx(maze,N) \n",
    "next_state = list(get_next_state(maze,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze Shape\n",
    "#### Assumptions:\n",
    "#### 1- 'S' -> Start State\n",
    "#### 2- '.' -> Normal State\n",
    "#### 3- '#' -> Barrier\n",
    "#### 4- 'E' -> End State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', '.', '.', '.', '#', '#']\n",
      "['#', '.', '#', '.', '.', '.']\n",
      "['.', '#', '.', '.', '.', '#']\n",
      "['.', '.', '.', 'E', '#', '#']\n",
      "['#', '.', '#', '.', '.', '.']\n",
      "['#', '.', '#', '.', '#', '.']\n"
     ]
    }
   ],
   "source": [
    "for ii in range(N):\n",
    "    print(maze[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Maze using Policy Iteration Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :\n",
      "Cuurent Values:\n",
      "[-1.         -1.         -1.         -1.         -0.33333333 -0.33333333\n",
      " -0.33333333 -1.         -0.33333333 -1.         -1.         -1.\n",
      " -1.         -0.33333333 -1.         -1.         -1.         -0.33333333\n",
      " -1.         -1.         -1.          0.         -0.33333333 -0.33333333\n",
      " -0.33333333 -1.         -0.33333333 -1.         -1.         -1.\n",
      " -0.33333333 -1.         -0.33333333 -1.         -0.33333333 -1.        ]\n",
      "Current Policy Probability distribution: \n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n",
      "Iteration  1 :\n",
      "Cuurent Values:\n",
      "[-2.         -2.         -2.         -2.         -0.66666667 -0.66666667\n",
      " -0.66666667 -2.         -0.66666667 -2.         -2.         -2.\n",
      " -2.         -0.66666667 -2.         -1.83333333 -2.         -0.66666667\n",
      " -2.         -2.         -1.83333333  0.         -0.66666667 -0.66666667\n",
      " -0.66666667 -2.         -0.66666667 -1.83333333 -2.         -2.\n",
      " -0.66666667 -2.         -0.66666667 -2.         -0.66666667 -2.        ]\n",
      "Current Policy Probability distribution: \n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n",
      "Iteration  2 :\n",
      "Cuurent Values:\n",
      "[-3.         -3.         -3.         -3.         -1.         -1.\n",
      " -1.         -3.         -1.         -2.97222222 -3.         -3.\n",
      " -3.         -1.         -2.94444444 -2.66666667 -2.97222222 -1.\n",
      " -3.         -2.97222222 -2.63888889  0.         -1.         -1.\n",
      " -1.         -3.         -1.         -2.63888889 -2.97222222 -3.\n",
      " -1.         -3.         -1.         -2.97222222 -1.         -3.        ]\n",
      "Current Policy Probability distribution: \n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n",
      "Iteration  3 :\n",
      "Cuurent Values:\n",
      "[-4.         -4.         -4.         -3.99537037 -1.33333333 -1.33333333\n",
      " -1.33333333 -4.         -1.33333333 -3.93981481 -3.99074074 -4.\n",
      " -4.         -1.33333333 -3.86574074 -3.48148148 -3.93518519 -1.33333333\n",
      " -3.99537037 -3.93518519 -3.42592593  0.         -1.33333333 -1.33333333\n",
      " -1.33333333 -3.99537037 -1.33333333 -3.43055556 -3.93055556 -3.99537037\n",
      " -1.33333333 -4.         -1.33333333 -3.92592593 -1.33333333 -4.        ]\n",
      "Current Policy Probability distribution: \n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]]\n",
      "Iteration  4 :\n",
      "Cuurent Values:\n",
      "[-5.         -5.         -4.9992284  -4.98842593 -1.66666667 -1.66666667\n",
      " -1.66666667 -5.         -1.66666667 -4.90123457 -4.97762346 -4.99845679\n",
      " -4.9992284  -1.66666667 -4.77314815 -4.29012346 -4.8904321  -1.66666667\n",
      " -4.98765432 -4.89197531 -4.20447531  0.         -1.66666667 -1.66666667\n",
      " -1.66666667 -4.98765432 -1.66666667 -4.21450617 -4.88117284 -4.98688272\n",
      " -1.66666667 -4.9992284  -1.66666667 -4.86805556 -1.66666667 -4.9992284 ]\n",
      "Current Policy Probability distribution: \n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]]\n",
      "Iteration  5 :\n",
      "Cuurent Values:\n",
      "[-6.         -5.9998714  -5.99781379 -5.97955247 -2.         -2.\n",
      " -2.         -6.         -2.         -5.8595679  -5.96129115 -5.99549897\n",
      " -5.99755658 -2.         -5.67348251 -5.0941358  -5.84143519 -2.\n",
      " -5.97775206 -5.84529321 -4.97826646  0.         -2.         -2.\n",
      " -2.         -5.97775206 -2.         -4.99395576 -5.82728909 -5.97569444\n",
      " -2.         -5.99755658 -2.         -5.80311214 -2.         -5.99742798]\n",
      "Current Policy Probability distribution: \n",
      "[[0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]]\n",
      "Iteration  6 :\n",
      "Cuurent Values:\n",
      "[-6.99997857 -6.9996142  -6.99584191 -6.96941444 -2.33333333 -2.33333333\n",
      " -2.33333333 -6.99997857 -2.33333333 -6.81575789 -6.94296553 -6.99129801\n",
      " -6.9950703  -2.33333333 -6.56989455 -5.8957476  -6.78971622 -2.33333333\n",
      " -6.96639232 -6.79651063 -5.74950703  0.         -2.33333333 -2.33333333\n",
      " -2.33333333 -6.96639232 -2.33333333 -5.77072617 -6.77070473 -6.96268433\n",
      " -2.33333333 -6.9950703  -2.33333333 -6.73388203 -2.33333333 -6.99466307]\n",
      "Current Policy Probability distribution: \n",
      "[[0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "\n",
      "-------------Final Results---------\n",
      "\n",
      "-\n",
      "Policy Probability Distribution:\n",
      "[[0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [1.         0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "Value Function:\n",
      "[-6.99997857 -6.9996142  -6.99584191 -6.96941444 -2.33333333 -2.33333333\n",
      " -2.33333333 -6.99997857 -2.33333333 -6.81575789 -6.94296553 -6.99129801\n",
      " -6.9950703  -2.33333333 -6.56989455 -5.8957476  -6.78971622 -2.33333333\n",
      " -6.96639232 -6.79651063 -5.74950703  0.         -2.33333333 -2.33333333\n",
      " -2.33333333 -6.96639232 -2.33333333 -5.77072617 -6.77070473 -6.96268433\n",
      " -2.33333333 -6.9950703  -2.33333333 -6.73388203 -2.33333333 -6.99466307]\n",
      "\n",
      "Path: \n",
      "[1, 2, 3, 9, 15, 21]\n",
      "Actions: \n",
      "['right', 'right', 'right', 'bottom', 'bottom', 'bottom']\n",
      "--- Running Time : 0.01604485511779785 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "policy, v = policy_improvement(reward, next_state , goal_index)\n",
    "exec_time = (time.time() - start_time)\n",
    "print(\"\\n\\n-------------Final Results---------\\n\\n-\")\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "path,actions = get_path_policy(policy)\n",
    "print(\"Path: \")\n",
    "print(path)\n",
    "print(\"Actions: \")\n",
    "print(actions)\n",
    "print(\"--- Running Time : %s seconds ---\" % exec_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Maze using Value Iteration Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1.  0.  0.  0. -1.  0. -1. -1. -1. -1.  0. -1. -1. -1.  0.\n",
      " -1. -1. -1.  0.  0.  0.  0. -1.  0. -1. -1. -1.  0. -1.  0. -1.  0. -1.]\n",
      "[-2. -2. -2. -2.  0.  0.  0. -2.  0. -2. -2. -2. -2.  0. -2. -1. -2.  0.\n",
      " -2. -2. -1.  0.  0.  0.  0. -2.  0. -1. -2. -2.  0. -2.  0. -2.  0. -2.]\n",
      "[-3. -3. -3. -3.  0.  0.  0. -3.  0. -2. -3. -3. -3.  0. -2. -1. -2.  0.\n",
      " -3. -2. -1.  0.  0.  0.  0. -3.  0. -1. -2. -3.  0. -3.  0. -2.  0. -3.]\n",
      "[-4. -4. -4. -3.  0.  0.  0. -4.  0. -2. -3. -4. -4.  0. -2. -1. -2.  0.\n",
      " -3. -2. -1.  0.  0.  0.  0. -3.  0. -1. -2. -3.  0. -4.  0. -2.  0. -4.]\n",
      "[-5. -5. -4. -3.  0.  0.  0. -5.  0. -2. -3. -4. -4.  0. -2. -1. -2.  0.\n",
      " -3. -2. -1.  0.  0.  0.  0. -3.  0. -1. -2. -3.  0. -4.  0. -2.  0. -4.]\n",
      "[-6. -5. -4. -3.  0.  0.  0. -6.  0. -2. -3. -4. -4.  0. -2. -1. -2.  0.\n",
      " -3. -2. -1.  0.  0.  0.  0. -3.  0. -1. -2. -3.  0. -4.  0. -2.  0. -4.]\n",
      "[-6. -5. -4. -3.  0.  0.  0. -6.  0. -2. -3. -4. -4.  0. -2. -1. -2.  0.\n",
      " -3. -2. -1.  0.  0.  0.  0. -3.  0. -1. -2. -3.  0. -4.  0. -2.  0. -4.]\n",
      "\n",
      "\n",
      "-------------Final Results---------\n",
      "\n",
      "-\n",
      "Value Function:\n",
      "[[-6. -5. -4. -3.  0.  0.]\n",
      " [ 0. -6.  0. -2. -3. -4.]\n",
      " [-4.  0. -2. -1. -2.  0.]\n",
      " [-3. -2. -1.  0.  0.  0.]\n",
      " [ 0. -3.  0. -1. -2. -3.]\n",
      " [ 0. -4.  0. -2.  0. -4.]]\n",
      "\n",
      "Path: \n",
      "[1, 2, 3, 9, 15, 21]\n",
      "Actions: \n",
      "['right', 'right', 'right', 'bottom', 'bottom', 'bottom']\n",
      "--- Running Time : 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mpolicy, v = value_iteration(N*N,goal_index)\n",
    "exec_time = (time.time() - start_time)\n",
    "print(\"\\n\\n-------------Final Results---------\\n\\n-\")\n",
    "print(\"Value Function:\")\n",
    "print(v.reshape((N,N)))\n",
    "print(\"\")\n",
    "path = []\n",
    "actions = []\n",
    "path,actions = get_path_policy(policy)\n",
    "print(\"Path: \")\n",
    "print(path)\n",
    "print(\"Actions: \")\n",
    "print(actions)\n",
    "print(\"--- Running Time : %s seconds ---\" % exec_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
